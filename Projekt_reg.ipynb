{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ScInglorion/AGH_ML_Course/blob/main/Projekt_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features in dataset:\n",
        "1. Age - age in years\n",
        "2. Sex - sex (1 male, 0 female)\n",
        "3. Chest pain type - (1 typical angina, 2 atypical angina, 3 non-anginal pain, 4 asymptomatic)\n",
        "4. BP\t- resting blood pressure (in mm Hg on admission to the hospital)\n",
        "5. Cholesterol - serum cholestoral in mg/dl\n",
        "6. FBS over 120\t- (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
        "7. EKG results - resting electrocardiographic results\n",
        "\n",
        "(0 normal,\n",
        "\n",
        "1 having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV),\n",
        "\n",
        "2 howing probable or definite left ventricular hypertrophy by Estes' criteria)\n",
        "8. Max HR\t- maximum heart rate achieved\n",
        "9. Exercise angina - exercise induced angina (1 = yes; 0 = no)\n",
        "10. ST depression\t- ST depression induced by exercise relative to rest\n",
        "11. Slope of ST\t- the slope of the peak exercise ST segment (1 upsloping, 2 flat, 3 downsloping)\n",
        "12. Number of vessels fluro\t- number of major vessels (0-3) colored by flourosopy\n",
        "13. Thallium - thalium (3 normal, 6 fixed defect, 7 reversable defect)\n",
        "14. Heart Disease - diagnosis of heart disease (angiographic disease status)\n",
        "        -- Value 0: < 50% diameter narrowing\n",
        "        -- Value 1: > 50% diameter narrowing\n"
      ],
      "metadata": {
        "id": "jplRrsdwwNsG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXwf3JJss_X1",
        "outputId": "be1aa932-0692-49c5-89a4-b86effc19b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Importing as many things i can\n",
        "import seaborn\n",
        "import pandas\n",
        "import numpy\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.preprocessing import (MinMaxScaler,StandardScaler)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "\n",
        "seaborn.set_theme() #seting the theme for the charts\n",
        "drive.mount('/content/drive')\n",
        "data = pandas.read_csv('/content/drive/MyDrive/Collab_project/Heart_Disease_Prediction.csv') #importing the data\n",
        "\n",
        "data.drop_duplicates() #droping duplicates\n",
        "data.dropna() #droping nan values\n",
        "\n",
        "#removing index (cuz useless), Heart Disease (we wanted to predict it), Number of vessels fluro (out target)\n",
        "Y = data['Number of vessels fluro']\n",
        "X = data.drop(['index', 'Heart Disease','Number of vessels fluro'], axis = 1)\n",
        "\n",
        "#changing non-binary categorical data to dummy variables\n",
        "X_after_dummies =  pandas.get_dummies(X, columns = ['Chest pain type', 'EKG results', 'Slope of ST', 'Thallium'])\n",
        "\n",
        "#spliting the data set in ratio 3:1:1 (Train:Validation:Test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_after_dummies, Y, test_size=0.2)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25)\n",
        "\n",
        "#removing outliers in Training set\n",
        "Y_train = Y_train.drop(X_train[X_train['Cholesterol']>500].index,axis = 0)\n",
        "X_train = X_train.drop(X_train[X_train['Cholesterol']>500].index,axis = 0)\n",
        "Y_train = Y_train.drop(X_train[X_train['ST depression']>=4.5].index,axis = 0)\n",
        "X_train = X_train.drop(X_train[X_train['ST depression']>=4.5].index,axis = 0)\n",
        "\n",
        "#creating normalized variables\n",
        "minmax_scaler = MinMaxScaler().set_output(transform=\"pandas\")\n",
        "X_train_normalized = minmax_scaler.fit_transform(X_train)\n",
        "X_val_normalized = minmax_scaler.fit_transform(X_val)\n",
        "X_test_normalized = minmax_scaler.fit_transform(X_test)\n",
        "\n",
        "#creating standardized variables\n",
        "standard_scaler = StandardScaler().set_output(transform=\"pandas\")\n",
        "X_train_standardized = standard_scaler.fit_transform(X_train[['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression']])\n",
        "X_val_standardized = standard_scaler.fit_transform(X_val[['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression']])\n",
        "X_test_standardized = standard_scaler.fit_transform(X_test[['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression']])\n",
        "\n",
        "X_train_standardized = pandas.concat([X_train_standardized, X_train.drop(['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression'], axis = 1)], axis=1)\n",
        "X_val_standardized = pandas.concat([X_val_standardized, X_val.drop(['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression'], axis = 1)], axis=1)\n",
        "X_test_standardized = pandas.concat([X_test_standardized, X_test.drop(['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression'], axis = 1)], axis=1)\n",
        "\n",
        "#creating PCA variables\n",
        "pca_train = PCA().set_output(transform=\"pandas\").fit(X_train_normalized)\n",
        "pca_val = PCA().set_output(transform=\"pandas\").fit(X_val_normalized)\n",
        "pca_test = PCA().set_output(transform=\"pandas\").fit(X_test_normalized)\n",
        "\n",
        "X_train_normalized_pca = pca_train.transform(X_train)\n",
        "X_val_normalized_pca = pca_val.transform(X_val)\n",
        "X_test_normalized_pca = pca_test.transform(X_test)\n",
        "\n",
        "X_train_normalized_pca = X_train_normalized_pca[['pca0','pca1','pca2','pca3']]\n",
        "X_val_normalized_pca = X_val_normalized_pca[['pca0','pca1','pca2','pca3']]\n",
        "X_test_normalized_pca = X_test_normalized_pca[['pca0','pca1','pca2','pca3']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##GridSearch code, for MLPRegressor, for different type of data\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
        "\n",
        "Y_train1 = pandas.concat([Y_train,Y_val])\n",
        "Y_test1 = Y_test\n",
        "\n",
        "#X_train1 = pandas.concat([X_train,X_val])\n",
        "#X_test1 = X_test\n",
        "\n",
        "#X_train1 = pandas.concat([X_train_normalized,X_val_normalized])\n",
        "#X_test1 = X_test_normalized\n",
        "\n",
        "#X_train1 = pandas.concat([X_train_standardized,X_val_standardized])\n",
        "#X_test1 = X_test_standardized\n",
        "\n",
        "X_train1 = pandas.concat([X_train_normalized_pca,X_val_normalized_pca])\n",
        "X_test1 = X_test_normalized_pca\n",
        "\n",
        "reg = MLPRegressor(random_state=1,max_iter=10000)\n",
        "parameters = {'hidden_layer_sizes':[(1),(2),(3),\n",
        "                                    (1,1),(1,2),(1,3),\n",
        "                                    (2,1),(2,2),(2,3),\n",
        "                                    (3,1),(3,2),(3,3),\n",
        "                                    (1,1,1),(1,1,2),(1,1,3),\n",
        "                                    (1,2,1),(1,2,2),(1,2,3),\n",
        "                                    (1,3,1),(1,3,2),(1,3,3),\n",
        "                                    (2,1,1),(2,1,2),(2,1,3),\n",
        "                                    (2,2,1),(2,2,2),(2,2,3),\n",
        "                                    (2,3,1),(2,3,2),(2,3,3),\n",
        "                                    (3,1,1),(3,1,2),(3,1,3),\n",
        "                                    (3,2,1),(3,2,2),(3,2,3),\n",
        "                                    (3,3,1),(3,3,2),(3,3,3)], 'activation':('identity', 'logistic', 'tanh', 'relu'), 'solver':('lbfgs', 'sgd', 'adam')}\n",
        "grid_search = GridSearchCV(reg, parameters, cv=5, scoring='neg_root_mean_squared_error')\n",
        "grid_search.fit(X_train1, Y_train1)"
      ],
      "metadata": {
        "id": "WTl0YhG9xEno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Printing best parameters after grid search\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "#==========================================================================================================================================\n",
        "# random split No. 1\n",
        "# for X_train and Y_train the result is:                {'activation': 'identity', 'hidden_layer_sizes': (2, 3, 3), 'solver': 'lbfgs'}\n",
        "# for X_train_normalized and Y_train the result is:     {'activation': 'identity', 'hidden_layer_sizes': (1, 3, 1), 'solver': 'lbfgs'}\n",
        "# for X_train_standardized and Y_train the result is:   {'activation': 'identity', 'hidden_layer_sizes': (2, 1, 2), 'solver': 'adam'}\n",
        "# for X_train_normalized_pca and Y_train the result is: {'activation': 'identity', 'hidden_layer_sizes': (2, 2, 2), 'solver': 'sgd'}\n",
        "\n",
        "#==========================================================================================================================================\n",
        "# random split No. 2\n",
        "# for X_train and Y_train the result is:                {'activation': 'identity', 'hidden_layer_sizes': 1, 'solver': 'lbfgs'}\n",
        "# for X_train_normalized and Y_train the result is:     {'activation': 'identity', 'hidden_layer_sizes': (2, 1, 1), 'solver': 'adam'}\n",
        "# for X_train_standardized and Y_train the result is:   {'activation': 'tanh', 'hidden_layer_sizes': 2, 'solver': 'adam'}\n",
        "# for X_train_normalized_pca and Y_train the result is: {'activation': 'tanh', 'hidden_layer_sizes': (2, 2), 'solver': 'lbfgs'}\n",
        "\n",
        "#==========================================================================================================================================\n",
        "# random split No. 3\n",
        "# for X_train and Y_train the result is:                {'activation': 'tanh', 'hidden_layer_sizes': (3, 2, 3), 'solver': 'adam'}\n",
        "# for X_train_normalized and Y_train the result is:     {'activation': 'tanh', 'hidden_layer_sizes': (3, 2, 2), 'solver': 'adam'}\n",
        "# for X_train_standardized and Y_train the result is:   {'activation': 'relu', 'hidden_layer_sizes': (3, 3), 'solver': 'adam'}\n",
        "# for X_train_normalized_pca and Y_train the result is: {'activation': 'identity', 'hidden_layer_sizes': (3, 3, 1), 'solver': 'lbfgs'}\n",
        "\n",
        "#==========================================================================================================================================\n",
        "# random split No. 4\n",
        "# for X_train and Y_train the result is:                {'activation': 'relu', 'hidden_layer_sizes': (3, 3, 1), 'solver': 'adam'}\n",
        "# for X_train_normalized and Y_train the result is:     {'activation': 'tanh', 'hidden_layer_sizes': (2, 3, 3), 'solver': 'adam'}\n",
        "# for X_train_standardized and Y_train the result is:   {'activation': 'relu', 'hidden_layer_sizes': (3, 3, 1), 'solver': 'adam'}\n",
        "# for X_train_normalized_pca and Y_train the result is: {'activation': 'logistic', 'hidden_layer_sizes': (3, 2, 2), 'solver': 'lbfgs'}\n",
        "\n",
        "#==========================================================================================================================================\n",
        "# random split No. 5\n",
        "# for X_train and Y_train the result is:                {'activation': 'identity', 'hidden_layer_sizes': (2, 1, 1), 'solver': 'lbfgs'}\n",
        "# for X_train_normalized and Y_train the result is:     {'activation': 'relu', 'hidden_layer_sizes': (2, 3, 2), 'solver': 'lbfgs'}\n",
        "# for X_train_standardized and Y_train the result is:   {'activation': 'tanh', 'hidden_layer_sizes': 2, 'solver': 'adam'}\n",
        "# for X_train_normalized_pca and Y_train the result is: {'activation': 'identity', 'hidden_layer_sizes': 2, 'solver': 'lbfgs'}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krzLr2BA0PC0",
        "outputId": "1197d3f2-e991-4e21-ef76-709aeff6fbc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'activation': 'identity', 'hidden_layer_sizes': 2, 'solver': 'lbfgs'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Code for checking the values of found model\n",
        "\n",
        "reg = MLPRegressor(hidden_layer_sizes=2, random_state=1, max_iter=10000, activation='identity', solver='lbfgs')\n",
        "reg.fit(X_train1, Y_train1)\n",
        "y_pred_train = reg.predict(X_train1)\n",
        "y_pred = reg.predict(X_test1)\n",
        "\n",
        "rmse = numpy.sqrt(numpy.mean((y_pred_train - Y_train1)**2))\n",
        "rmse_testing = numpy.sqrt(numpy.mean((y_pred - Y_test1)**2))\n",
        "print(\"rmse:\",rmse)\n",
        "print(\"rmse_testing:\",rmse_testing)\n",
        "\n",
        "#==========================================================================================================================================\n",
        "# random test split No. 1\n",
        "# for X_train and Y_train the results are:\n",
        "# rmse:         0.8437156423568695\n",
        "# rmse_testing: 0.7247217799916873\n",
        "#\n",
        "# for X_train_normalized and Y_train the results are:\n",
        "# rmse:         0.8294705258213844\n",
        "# rmse_testing: 0.7308239496694735\n",
        "#\n",
        "# for X_train_standardized and Y_train the results are:\n",
        "# rmse:         0.8590157493087447\n",
        "# rmse_testing: 0.6891325214377091\n",
        "#\n",
        "# for X_train_normalized_pca and Y_train the results are:\n",
        "# rmse:         0.9220446085288443\n",
        "# rmse_testing: 0.8653325743978505\n",
        "\n",
        "#==========================================================================================================================================\n",
        "# random test split No. 2\n",
        "# for X_train and Y_train the results are:\n",
        "# rmse:         0.8161560423140588\n",
        "# rmse_testing: 0.7722442462028298\n",
        "#\n",
        "# for X_train_normalized and Y_train the results are:\n",
        "# rmse:         0.8815112406923867\n",
        "# rmse_testing: 0.8075814924281434\n",
        "#\n",
        "# for X_train_standardized and Y_train the results are:\n",
        "# rmse:         0.8287945582816887\n",
        "# rmse_testing: 0.8287945582816887\n",
        "#\n",
        "# for X_train_normalized_pca and Y_train the results are:\n",
        "# rmse:         0.8738887359251216\n",
        "# rmse_testing: 0.8384768089707256\n",
        "\n",
        "#==========================================================================================================================================\n",
        "# random test split No. 3\n",
        "# for X_train and Y_train the results are:\n",
        "# rmse:         0.8122700745735869\n",
        "# rmse_testing: 0.9454545403780725\n",
        "#\n",
        "# for X_train_normalized and Y_train the results are:\n",
        "# rmse:         0.8381347571273422\n",
        "# rmse_testing: 0.8889473437882617\n",
        "#\n",
        "# for X_train_standardized and Y_train the results are:\n",
        "# rmse:         0.7049813477125647\n",
        "# rmse_testing: 0.9219754811209938\n",
        "#\n",
        "# for X_train_normalized_pca and Y_train the results are:\n",
        "# rmse:         0.8513141289478895\n",
        "# rmse_testing: 0.8981381122997653\n",
        "\n",
        "#==========================================================================================================================================\n",
        "# random test split No. 4\n",
        "# for X_train and Y_train the results are:\n",
        "# rmse:         0.8525801241722037\n",
        "# rmse_testing: 1.216769617099928\n",
        "#\n",
        "# for X_train_normalized and Y_train the results are:\n",
        "# rmse:         0.8098487748307407\n",
        "# rmse_testing: 0.8535397608337245\n",
        "#\n",
        "# for X_train_standardized and Y_train the results are:\n",
        "# rmse:         0.6673387337019309\n",
        "# rmse_testing: 0.9323472637368998\n",
        "#\n",
        "# for X_train_normalized_pca and Y_train the results are:\n",
        "# rmse:         0.8205308935067461\n",
        "# rmse_testing: 0.8719884717963478\n",
        "\n",
        "#==========================================================================================================================================\n",
        "# random test split No. 5\n",
        "# for X_train and Y_train the results are:\n",
        "# rmse:         0.7889453332281064\n",
        "# rmse_testing: 0.9669756792721176\n",
        "#\n",
        "# for X_train_normalized and Y_train the results are:\n",
        "# rmse:         0.7047483413401348\n",
        "# rmse_testing: 0.9439886918941585\n",
        "#\n",
        "# for X_train_standardized and Y_train the results are:\n",
        "# rmse:         0.8075535523522982\n",
        "# rmse_testing: 0.9014995363905408\n",
        "#\n",
        "# for X_train_normalized_pca and Y_train the results are:\n",
        "# rmse:         0.8363856159846348\n",
        "# rmse_testing: 0.9692139863849197"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNj8BDcj3A9G",
        "outputId": "803c1184-7f22-4554-d67a-ce7dc5495a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 0.8363856159846348\n",
            "rmse_testing: 0.9692139863849197\n"
          ]
        }
      ]
    }
  ]
}